from keras.layers import Conv2D, LeakyReLu, BatchNormalization, MaxPooling2D,
Dense, Dropout, InputLayer
from keras.models import Sequential
from keras.activations import softmax

#load the dataset (argument parse) - it will look something like this
#dataset = loadtxt("filename", delimiter=",")
#splitting into input (inp) and output (out) variables -this will be an x and y
#type of situation with e.g. x or y = dataset[0:9]

# PROMOTERS MODEL

pmodel = Sequential()
#INPUT DENSE LAYER
pmodel.add(Dense(22, input_shape=(16,)))
# CONVOLUTIONAL LAYER 1 
lrelu = lambda x: keras.layers.LeakyReLu(x, alpha=0.1)
pmodel.add(Conv2D(filters, kernel_size, strides=(1, 1),padding='valid', data_format="channels_first", dilation_rate=(1, 1),activation="lrelu", use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros'))
# BATCH NORMALISATION
pmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer='zeros',gamma_initializer='ones', moving_mean_initializer='zeros',moving_variance_initializer='ones'))
# MAX POOLING
pmodel.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
# CONVOLUTIONAL LAYER 2 
pmodel.add(Conv2D(filters, kernel_size, strides=(1, 1),padding='valid', data_format="channels_first", dilation_rate=(1, 1),activation="lrelu", use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros'))
# BATCH NORMALISATION
pmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer='zeros',gamma_initializer='ones', moving_mean_initializer='zeros',moving_variance_initializer='ones'))
# CONVOLUTIONAL LAYER 3 
pmodel.add(Conv2D(filters, kernel_size, strides=(1, 1),padding='valid', data_format="channels_first", dilation_rate=(1, 1),activation="lrelu", use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros'))
# BATCH NORMALISATION
pmodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer='zeros',gamma_initializer='ones', moving_mean_initializer='zeros',moving_variance_initializer='ones'))
# MAX POOLING
pmodel.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
# DENSE LAYER 
pmodel.add(Dense(units, activation=lrelu, use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))


# ENHANCERS MODEL

emodel = Sequential()
#INPUT DENSE LAYER
emodel.add(Dense(22, input_shape=(16,)))
# CONVOLUTIONAL LAYER 1 
lrelu = lambda x: keras.layers.LeakyReLu(x, alpha=0.1)
emodel.add(Conv2D(filters, kernel_size, strides=(1, 1),padding='valid', data_format="channels_first", dilation_rate=(1, 1),activation="lrelu", use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros'))
# BATCH NORMALISATION
emodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer='zeros',gamma_initializer='ones', moving_mean_initializer='zeros',moving_variance_initializer='ones'))# MAX POOLING
emodel.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
# CONVOLUTIONAL LAYER 2 
emodel.add(Conv2D(filters, kernel_size, strides=(1, 1),padding='valid', data_format="channels_first", dilation_rate=(1, 1),activation="lrelu", use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros'))
# BATCH NORMALISATION
emodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer='zeros',gamma_initializer='ones', moving_mean_initializer='zeros',moving_variance_initializer='ones'))
# CONVOLUTIONAL LAYER 3 
emodel.add(Conv2D(filters, kernel_size, strides=(1, 1),padding='valid', data_format="channels_first", dilation_rate=(1, 1),activation="lrelu", use_bias=True,kernel_initializer='glorot_uniform', bias_initializer='zeros'))
# BATCH NORMALISATION
emodel.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True,beta_initializer='zeros',gamma_initializer='ones', moving_mean_initializer='zeros',moving_variance_initializer='ones'))
# MAX POOLING
emodel.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
# DENSE LAYER
emodel.add(Dense(units, activation=lrelu, use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))


# MERGING EMODEL AND PMODEL
from keras.layers.merge import Concatenate
# concatenating models 
merged = Concatenate([pmodel, emodel])
# Dense layer
merged.add(Dense(units, input_shape=(1,), activation="ReLU"))
# ADD DROPOUT - 20% of weights removed before softmax is applied
merged.add(Dropout(0.2, noise_shape=None, seed=None))
# DENSE LAYER with softmax activation function
merged.add(Dense(1, activation="softmax", use_bias=True, kernel_initializer='glorot_uniform',bias_initializer='zeros'))

# COMPILATION - import Adam separately from optimizers
merged.compile(optimizer="Adam", loss="categorical_crossentropy", metrics=["accuracy"])




#TRAINING
#x is the numpy array of training data if model has single input, or a list of numpy arrays
#if the model has mutliple inputs.
#y is the numpy array of target (label) data same format as above

merged.fit(fit(x=None, y=None, batch_size=20, epochs=200, verbose=1, callbacks=None,
               validation_split=0.15, shuffle=True, class_weight=None,
               sample_weight=None, initial_epoch=0, steps_per_epoch=None,
               validation_freq=1))

# EVALUATION - something like this
_, accuracy = merged.evaluate(X, y)
print("Accuracy: %.2f" % (accuracy*100))
